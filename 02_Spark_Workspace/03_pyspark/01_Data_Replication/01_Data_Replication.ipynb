{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName='Data Replication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'a', '1'], ['a', 'a', '2'], ['a', 'b', '1']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_1 = sc.textFile(\"hdfs:///user/root/01_multiply_data_input.tsv\").map(lambda x: x.split(\"\\t\"))\n",
    "rdd_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2 = sc.parallelize([str(i+1) for i in range(2)])\n",
    "rdd_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    l = []\n",
    "    l.append(x[0][0])\n",
    "    l.append(x[0][1] + \"_\" + x[1])\n",
    "    l.append(x[0][2])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'a_1', '1'],\n",
       " ['a', 'a_1', '2'],\n",
       " ['a', 'a_2', '1'],\n",
       " ['a', 'a_2', '2'],\n",
       " ['a', 'b_1', '1'],\n",
       " ['a', 'b_2', '1']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_3 = rdd_1.cartesian(rdd_2).map(lambda x: f(x))\n",
    "rdd_3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return \"\\t\".join(x[i] for i in range(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a\\ta_1\\t1', 'a\\ta_1\\t2', 'a\\ta_2\\t1', 'a\\ta_2\\t2', 'a\\tb_1\\t1', 'a\\tb_2\\t1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_4 = rdd_3.map(lambda x: g(x))\n",
    "rdd_4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(rdd_1, \"toDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(rdd_1, \"toDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'a', '1'), ('a', 'a', '2'), ('a', 'b', '1')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_1.map(lambda x: (x[0], x[1], x[2])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|col_1|col_2|col_3|\n",
      "+-----+-----+-----+\n",
      "|    a|    a|    1|\n",
      "|    a|    a|    2|\n",
      "|    a|    b|    1|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = rdd_1.map(lambda x: (x[0], x[1], x[2])).toDF([\"col_1\", \"col_2\", \"col_3\"])\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|col_1|col_2|col_3|\n",
      "+-----+-----+-----+\n",
      "|    a|  a_1|    1|\n",
      "|    a|  a_1|    2|\n",
      "|    a|  a_2|    1|\n",
      "|    a|  a_2|    2|\n",
      "|    a|  b_1|    1|\n",
      "|    a|  b_2|    1|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = rdd_3.map(lambda x: (x[0], x[1], x[2])).toDF([\"col_1\", \"col_2\", \"col_3\"])\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "Deleted /user/root/01_multiply_data_output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rmr /user/root/01_multiply_data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_4.saveAsTextFile(\"hdfs:///user/root/01_multiply_data_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   1 root supergroup          0 2019-11-09 16:58 /user/root/01_multiply_data_output/_SUCCESS\r\n",
      "-rw-r--r--   1 root supergroup         16 2019-11-09 16:58 /user/root/01_multiply_data_output/part-00000\r\n",
      "-rw-r--r--   1 root supergroup         16 2019-11-09 16:58 /user/root/01_multiply_data_output/part-00001\r\n",
      "-rw-r--r--   1 root supergroup          8 2019-11-09 16:58 /user/root/01_multiply_data_output/part-00002\r\n",
      "-rw-r--r--   1 root supergroup          8 2019-11-09 16:58 /user/root/01_multiply_data_output/part-00003\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/root/01_multiply_data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\ta_1\t1\r\n",
      "a\ta_1\t2\r\n",
      "a\ta_2\t1\r\n",
      "a\ta_2\t2\r\n",
      "a\tb_1\t1\r\n",
      "a\tb_2\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/root/01_multiply_data_output/part-0000[0-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
